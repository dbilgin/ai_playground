{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Text Generation With DB Playground  \n",
    "Text generation with Mistral 7B using sample DB data.\n",
    "\n",
    "1. **Run the first cell** to install requirements.  \n",
    "2. Switch the runtime to **GPU**. If running on Colab Runtime ‚Üí Change runtime type ‚Üí T4 GPU.\n",
    "3. Enter a prompt to test the model.\n",
    "\n",
    "> Model: *[Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r https://raw.githubusercontent.com/dbilgin/ai_playground/refs/heads/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "# ---------- tiny ‚ÄúDB‚Äù ----------\n",
    "KB = [\n",
    "    {\"id\": 1, \"text\": \"Transformers were introduced in the paper 'Attention Is All You Need (2017)'.\"},\n",
    "    {\"id\": 2, \"text\": \"Gradient descent adjusts model weights to minimise loss.\"},\n",
    "    {\"id\": 3, \"text\": \"The Adam optimiser combines momentum and adaptive learning-rates.\"},\n",
    "]\n",
    "embedder   = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kb_vectors = embedder.encode([d[\"text\"] for d in KB], normalize_embeddings=True)\n",
    "\n",
    "SYSTEM = \"\"\"You are a concise AI tutor; use the supplied context if it helps.\"\"\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tok  = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"load_in_4bit\": True,\n",
    "        \"device_map\": {\"\": 0},\n",
    "        \"max_memory\": {0: \"10GiB\", \"cpu\": \"32GiB\"},\n",
    "        \"torch_dtype\": \"auto\"\n",
    "    }\n",
    ")\n",
    "\n",
    "SIM_TH = 0.30\n",
    "\n",
    "def fetch_ctx(question, k=2):\n",
    "    q = embedder.encode([question], normalize_embeddings=True)\n",
    "    sims = (kb_vectors @ q.T).squeeze()          # cosine similarity\n",
    "    best = sims.argsort()[-k:][::-1]             # top-k ids\n",
    "    if sims[best[0]] < SIM_TH:                  # nothing close enough\n",
    "        return \"\"\n",
    "    return \"\\n\".join(KB[i][\"text\"] for i in best)\n",
    "\n",
    "def chat(msg, history=None):\n",
    "    ctx = fetch_ctx(msg)\n",
    "    gr.Warning(\"***\" + ctx + \"***\")\n",
    "    history = history or []\n",
    "    messages = [{\"role\":\"system\",\"content\":SYSTEM}]\n",
    "    for u,a in history:\n",
    "        messages += [{\"role\":\"user\",\"content\":u},{\"role\":\"assistant\",\"content\":a}]\n",
    "    \n",
    "    if not ctx:\n",
    "        messages.append({\"role\":\"user\",\"content\":f\"{ctx}\\n\\nQ: {msg}\"})\n",
    "    else:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context:\\n{ctx}\\n\\nAnswer **only** from the context without changing it. {msg}\"\n",
    "        })\n",
    "\n",
    "    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    reply  = pipe(prompt, max_new_tokens=128, eos_token_id=tok.eos_token_id)[0][\"generated_text\"][len(prompt):].strip()\n",
    "    history.append((msg, reply))\n",
    "    return history, history\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chat,\n",
    "    inputs=[\"text\", gr.State()],\n",
    "    outputs=[gr.Chatbot(), gr.State()],\n",
    ").launch(debug=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
