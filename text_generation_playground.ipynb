{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ–¼ï¸ Text Generation Playground  \n",
    "Text generation with Mistral 7B.\n",
    "\n",
    "1. **Run the first cell** to install requirements.  \n",
    "2. Switch the runtime to **GPU**. If running on Colab Runtime â†’ Change runtime type â†’ T4 GPU.\n",
    "3. Enter a prompt to test the model.\n",
    "\n",
    "> Model: *[Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r https://raw.githubusercontent.com/dbilgin/ai_playground/refs/heads/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.pipelines import pipeline\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)\n",
    "\n",
    "SYSTEM = \"\"\"You are a concise AI tutor; use the supplied context if it helps.\"\"\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tok  = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"load_in_4bit\": True,\n",
    "        \"device_map\": {\"\": 0},\n",
    "        \"max_memory\": {0: \"10GiB\", \"cpu\": \"32GiB\"},\n",
    "        \"torch_dtype\": \"auto\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def chat(msg, history=None):\n",
    "    history = history or []\n",
    "    messages = [{\"role\":\"system\",\"content\": SYSTEM}]\n",
    "    for u, a in history:                  # older turns\n",
    "        messages += [{\"role\":\"user\",\"content\":u},\n",
    "                     {\"role\":\"assistant\",\"content\":a}]\n",
    "    messages.append({\"role\":\"user\",\"content\":msg})   # new user line\n",
    "\n",
    "    prompt = tok.apply_chat_template(messages,\n",
    "                                     tokenize=False,\n",
    "                                     add_generation_prompt=True)\n",
    "    gen = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )[0][\"generated_text\"]\n",
    "    reply = gen[len(prompt):].strip()\n",
    "    history.append((msg, reply))\n",
    "    return history, history\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chat,\n",
    "    inputs=[\"text\", gr.State()],\n",
    "    outputs=[gr.Chatbot(), gr.State()],\n",
    ").launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train imdb\n",
    "\n",
    "- Run the below cell to train the [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) model on imdb data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)\n",
    "\n",
    "def format_prompts(examples):\n",
    "    return {\n",
    "        \"text\": [\n",
    "            f\"### Review:\\n{review}\\n### Sentiment:\"\n",
    "            for review in examples[\"text\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "dataset['text'][2] # Check to see if the fields were formatted correctly\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"trained_models/text-generation\",\n",
    "#     num_train_epochs=4, # replace this, depending on your dataset\n",
    "#     per_device_train_batch_size=16,\n",
    "#     learning_rate=1e-5,\n",
    "#     optim=\"sgd\"\n",
    "# )\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"trained_models/mistral-imdb\",\n",
    "    max_length=256,           # shorter context â†’ far less KV-cache\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=lambda row: row[\"text\"],\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "adapter_model = trainer.model\n",
    "merged_model = adapter_model.merge_and_unload()\n",
    "\n",
    "trained_tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with databricks-dolly-15k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)\n",
    "\n",
    "def format_prompts(example):\n",
    "    context = example.get(\"input\", \"\")\n",
    "    return (\n",
    "        f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "        f\"### Context:\\n{context}\\n\\n\"\n",
    "        f\"### Response:\\n{example['output']}\"\n",
    "    )\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train\")\n",
    "# dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "\n",
    "# dataset = dataset.map(format_prompts, batched=True) // removed in favor of format_prompts\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"trained_models/mistral-dolly-15k\",\n",
    "    max_length=256,           # shorter context â†’ far less KV-cache\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=format_prompts,\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "adapter_model = trainer.model\n",
    "merged_model = adapter_model.merge_and_unload()\n",
    "\n",
    "trained_tokenizer = trainer.tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
