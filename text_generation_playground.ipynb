{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ–¼ï¸ Text Generation Playground  \n",
    "Text generation with Mistral 7B.\n",
    "\n",
    "1. **Run the first cell** to install requirements.  \n",
    "2. Switch the runtime to **GPU**. If running on Colab Runtime â†’ Change runtime type â†’ T4 GPU.\n",
    "3. Enter a prompt to test the model.\n",
    "\n",
    "> Model: *[Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r https://raw.githubusercontent.com/dbilgin/ai_playground/refs/heads/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.pipelines import pipeline\n",
    "\n",
    "SYSTEM = \"\"\"You are a concise AI tutor; use the supplied context if it helps.\"\"\"\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tok  = AutoTokenizer.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"load_in_4bit\": True,\n",
    "        \"device_map\": {\"\": 0},\n",
    "        \"max_memory\": {0: \"10GiB\", \"cpu\": \"32GiB\"},\n",
    "        \"torch_dtype\": \"auto\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def chat(msg, history=None):\n",
    "    history = history or []\n",
    "    messages = [{\"role\":\"system\",\"content\": SYSTEM}]\n",
    "    for u, a in history:                  # older turns\n",
    "        messages += [{\"role\":\"user\",\"content\":u},\n",
    "                     {\"role\":\"assistant\",\"content\":a}]\n",
    "    messages.append({\"role\":\"user\",\"content\":msg})   # new user line\n",
    "\n",
    "    prompt = tok.apply_chat_template(messages,\n",
    "                                     tokenize=False,\n",
    "                                     add_generation_prompt=True)\n",
    "    gen = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )[0][\"generated_text\"]\n",
    "    reply = gen[len(prompt):].strip()\n",
    "    history.append((msg, reply))\n",
    "    return history, history\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chat,\n",
    "    inputs=[\"text\", gr.State()],\n",
    "    outputs=[gr.Chatbot(), gr.State()],\n",
    ").launch(debug=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "- Run the below cell to train the [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) model on imdb data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "def format_prompts(examples):\n",
    "    return {\n",
    "        \"text\": [\n",
    "            f\"### Review:\\n{review}\\n### Sentiment:\"\n",
    "            for review in examples[\"text\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "dataset['text'][2] # Check to see if the fields were formatted correctly\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"trained_models/text-generation\",\n",
    "#     num_train_epochs=4, # replace this, depending on your dataset\n",
    "#     per_device_train_batch_size=16,\n",
    "#     learning_rate=1e-5,\n",
    "#     optim=\"sgd\"\n",
    "# )\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"trained_models/mistral-imdb\",\n",
    "    max_length=256,           # shorter context â†’ far less KV-cache\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=lambda row: row[\"text\"],\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "adapter_model = trainer.model\n",
    "merged_model = adapter_model.merge_and_unload()\n",
    "\n",
    "trained_tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with databricks-dolly-15k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:00<00:00, 1007772.47 examples/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 307.00 MiB is free. Including non-PyTorch memory, this process has 10.20 GiB memory in use. Of the allocated memory 9.83 GiB is allocated by PyTorch, and 140.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n\u001b[32m     29\u001b[39m model.gradient_checkpointing_enable()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m model = \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m config = LoraConfig(\n\u001b[32m     33\u001b[39m     r=\u001b[32m32\u001b[39m,\n\u001b[32m     34\u001b[39m     lora_alpha=\u001b[32m64\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m )\n\u001b[32m     41\u001b[39m model = get_peft_model(model, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai_playground/.env/lib/python3.12/site-packages/peft/utils/other.py:141\u001b[39m, in \u001b[36mprepare_model_for_kbit_training\u001b[39m\u001b[34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    139\u001b[39m             (param.dtype == torch.float16) \u001b[38;5;129;01mor\u001b[39;00m (param.dtype == torch.bfloat16)\n\u001b[32m    140\u001b[39m         ) \u001b[38;5;129;01mand\u001b[39;00m param.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mParams4bit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m             param.data = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    144\u001b[39m     loaded_in_kbit\n\u001b[32m    145\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m is_gptq_quantized\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m ) \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m gradient_checkpointing_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    153\u001b[39m         \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 307.00 MiB is free. Including non-PyTorch memory, this process has 10.20 GiB memory in use. Of the allocated memory 9.83 GiB is allocated by PyTorch, and 140.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "def format_prompts(example):\n",
    "    context = example.get(\"input\", \"\")\n",
    "    return (\n",
    "        f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "        f\"### Context:\\n{context}\\n\\n\"\n",
    "        f\"### Response:\\n{example['output']}\"\n",
    "    )\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "\n",
    "# dataset = dataset.map(format_prompts, batched=True) // removed in favor of format_prompts\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "cfg = SFTConfig(\n",
    "    output_dir=\"trained_models/mistral-dolly-15k\",\n",
    "    max_length=256,           # shorter context â†’ far less KV-cache\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=format_prompts,\n",
    "    args=cfg,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "adapter_model = trainer.model\n",
    "merged_model = adapter_model.merge_and_unload()\n",
    "\n",
    "trained_tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "print(fsspec.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
